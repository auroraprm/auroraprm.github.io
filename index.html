<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="AURORA"/>
  <meta property="og:description" content="AURORA: Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AURORA</title>
  <link rel="icon" type="image/x-icon" href="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Aurora_borealis_over_Eielson_Air_Force_Base%2C_Alaska.jpg/2560px-Aurora_borealis_over_Eielson_Air_Force_Base%2C_Alaska.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['\\(','\\)']], // 行内公式标记
        displayMath: [['\\[','\\]']] // 块级公式标记
      }
    };
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AURORA</h1>
            <h1 class="title is-2 publication-title">Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Xiaoyu Tan<sup>1,*,‡</sup>,</span>
              <span class="author-block">
                Tianchu Yao<sup>2,*</sup>,</span>
              <span class="author-block">
                Chao Qu<sup>2,*</sup>,</span>
              <span class="author-block">
                Bin Li<sup>3,*</sup>,</span>
              <span class="author-block">
                Minghao Yang<sup>2</sup>,</span>
              <span class="author-block">
                Dakuan Lu<sup>2</sup>,</span>
              <span class="author-block">
                Haozhe Wang<sup>4</sup>,</span>
              <span class="author-block">
                Xu Yinghui<sup>2</sup>,</span>
              <span class="author-block">
                Xihe Qiu <sup>3,†</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Tencent Youtu Lab,</span>
              <span class="author-block"><sup>2</sup>Fudan University,</span>
              <span class="author-block"><sup>3</sup>Shanghai University of Engineering Science,</span>
              <span class="author-block"><sup>4</sup>The Hong Kong University of Science and Technology</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution</span>
              <span class="author-block"><sup>†</sup>Corresponding author: qiuxihe1993@gmail.com</span>
              <br>
              <span class="author-block"><sup>‡</sup>The first author is currently affiliated with Tencent Youtu Lab. This work was partially conducted while the author was at INF Technology (Shanghai) Co., Ltd.</span>
            </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://huggingface.co/universalprm/Universal-PRM" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="width: 20px; height: 20px;">
                       </span>
                        <span>Model</span>
                      </a>
                    </span>

                    <!-- Arxiv link -->
<!--                     <span class="link-block">
                      <a href="https://huggingface.co/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Arxiv</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="is-two-thirds">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified is-size-5">
          <p>
            The reasoning capabilities of advanced large language models (LLMs) like o1 have revolutionized artificial intelligence applications. Nevertheless, evaluating and optimizing complex reasoning processes remain significant challenges due to diverse policy distributions and the inherent limitations of human effort and accuracy. In this paper, we present AURORA, a novel automated framework for training universal process reward models (PRMs) using ensemble prompting and reverse verification. The framework employs a two-phase approach: First, it uses diverse prompting strategies and ensemble methods to perform automated annotation and evaluation of processes, ensuring robust assessments for reward learning. Second, it leverages practical reference answers for reverse verification, enhancing the model’s ability to validate outputs and improving training accuracy. To assess the framework’s performance, we extend beyond the existing ProcessBench benchmark by introducing UniversalBench, which evaluates reward predictions across full trajectories under diverse policy distribtion with long Chain-of-Thought (CoT) outputs. Experimental results demonstrate that AURORA enhances process evaluation accuracy, improves PRMs' accuracy for diverse policy distributions and long-CoT responses. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="is-two-thirds">
        <h2 class="title is-3">Overall Workflow of AURORA</h2>
        <div class="content has-text-justified is-size-5">
           <p>
            <img src="static/images/framework_v2.png" alt="The overall workflow of AURORA." class="width: 66%; height: auto;">
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="is-two-thirds">
        <h2 class="title is-3">Experiment</h2>
        <div class="content has-text-justified is-size-5">
          <p>
            We evaluate the performance of the universal PRM trained using our proposed AURORA. First, we assess the effectiveness of our framework on ProcessBench, a benchmark specifically designed to evaluate generation processes using human-annotated labels. Next, we investigate the universal capabilities of the trained PRM across diverse policy distributions. To facilitate this evaluation, we construct a novel dataset called UniversalBench that spans a wide range of policy distributions, varying in both sequence length and step separation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <div class="image-container">
        <img src="static/images/figure1.png" alt="MY ALT TEXT"/>
      </div>
        <h2 class="subtitle has-text-centered">
          The experiment results on ProcessBench.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <div class="image-container">
        <img src="static/images/figure3.png" alt="MY ALT TEXT"/>
      </div>
        <h2 class="subtitle has-text-centered">
          The experiment results on UniversalBench.
        </h2>
      </div>
        <div class="item">
        <!-- Your image here -->
        <div class="image-container">
        <img src="static/images/ablation1.png" alt="MY ALT TEXT"/>
      </div>
        <h2 class="subtitle has-text-centered">
          The experiment results of ablation.
        </h2>
      </div>
        <div class="item">
        <!-- Your image here -->
        <div class="image-container">
        <img src="static/images/ablation2.png" alt="MY ALT TEXT"/>
      </div>
        <h2 class="subtitle has-text-centered">
          The experiment results of ablation.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="is-two-thirds">
        <div class="content has-text-justified is-size-5">
          <p>ProcessBench experimental results shown in Table 1 demonstrate that Universal-PRM-7B, trained using our proposed AURORA, achieves superior performance on the ProcessBench benchmark, excelling in both the overall average score and evaluations across four subsets. These results highlight the effectiveness of our approach in generalization under a universal policy training distribution and underscore the robustness of our proposed ensemble prompting techniques.
            </p>
          <p>
            Experimental results, as shown in the Table 2, demonstrate that UniversalBench training under our proposed AURORA framework has achieved superior performance, highlighting its strong generalization capabilities across diverse policy distributions. By training under the AURORA framework, UniversalBench effectively addresses the challenges by constructing \(\mathcal{D}_{\text{gen}}\) using diverse policy and prompt distributions, particularly containing long CoT reasoning. This indicates the robustness of our approach in capturing and adapting to a wide range of policy behaviors, thereby outperforming existing methods in accuracy, making it applicable to real-world scenarios where the update policy distributions are dynamic. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="is-two-thirds">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified is-size-5">
          <p>
            In this paper, we introduce a novel framework, AURORA, designed for automated process reward labeling and learning using LLMs. Unlike recent approaches \cite{zheng2024processbench, lightman2023let} that operate on limited data distributions, rely solely on questions and partial solutions, or focus only on the first error occurrence, AURORA aims to train universal PRMs by addressing these limitations. Specifically, AURORA collects candidate reasoning trajectories from diverse policy distributions, evaluates process rewards across the entire reasoning sequence to support downstream RL algorithms, and incorporates reverse verification and ensemble prompting techniques to further enhance performance. To comprehensively evaluate our approach, we curated a new benchmark, UniversalBench, which captures a wide range of policy distribution and especially contains long CoT policy outputs that closely mirror real-world PRM usage scenarios in optimizing long CoT policies. Experiments on both ProcessBench and UniversalBench demonstrate that Universal-PRM-7B, trained using AURORA, achieves SOTA performance. We have open-sourced Universal-PRM-7B and UniversalBench to encourage community adoption and further research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>








<!-- Youtube video -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Prompt Details</h2>
        
        <div class="tabs is-centered is-boxed is-toggle">
          <ul>
            <li class="is-active" data-tab="prompt-gen">
              <a>
                <span>Prompt Set \(\mathcal{P}_{\text{gen}}\)</span>
              </a>
            </li>
            <li data-tab="prompt-sep">
              <a>
                <span>Step Separation \(p_s\)</span>
              </a>
            </li>
            <li data-tab="prompt-dis">
              <a>
                <span>Prompt Set \(\mathcal{P}_{\text{dis}}\)</span>
              </a>
            </li>
          </ul>
        </div>

        <div id="prompt-gen" class="content-tab">
          <div class="content">
            <article class="message">
              <div class="message-header">
                <p>Default system prompt</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                You are a helpful assistant.
              </div>
            </article>

            <article class="message">
              <div class="message-header">
                <p>QwQ system prompt</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.
              </div>
            </article>

            <article class="message">
              <div class="message-header">
                <p>INF-o1 \(\pi_0\) system prompt</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                You are an advanced AI language model specializing in solving math and programming problems step by step. Carefully analyze each part of the problem, verify the accuracy of your reasoning with relevant facts and data, and provide clear, logical solutions. Reflect on and review your approach throughout the problem-solving process to ensure precision and thoroughness. Always think through the problem step by step and provide your answers accordingly.
              </div>
            </article>

            <article class="message">
              <div class="message-header">
                <p>Question prompt \(p_{0}\)</p>
              </div>
              <div class="message-body">
                <strong>[User]:</strong><br>
                <span style="color: red;">{question}</span>
              </div>
            </article>

            <article class="message">
              <div class="message-header">
                <p>Question prompt \(p_{1}\)</p>
              </div>
              <div class="message-body">
                <strong>[User]:</strong><br>
                <span style="color: red;">{question}</span><br>
                Let's think step by step.
              </div>
            </article>

            <article class="message">
              <div class="message-header">
                <p>Question prompt \(p_{2}\)</p>
              </div>
              <div class="message-body">
                <strong>[User]:</strong><br>
                <span style="color: red;">{question}</span><br>
                First, deeply analyze the problem and identify key concepts and relationships, then solve it step by step with clear reasoning.
              </div>
            </article>
          </div>
        </div>

        <div id="prompt-sep" class="content-tab is-hidden">
          <div class="content">
            <article class="message">
              <div class="message-header">
                <p>LLM discrimination prompt \(p_s\)</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                You are a helpful assistant who can seperate the logical steps accurately.<br>
                <hr>
                <strong>[User]:</strong><br>
                Please split the following math problem-solving text according to logical 
                steps, generating a JSON object where each step is an independent key-value 
                pair in the format `{{ "Step X": "Content of the step" }}`, where `X` is the step number. Note:<br>
                <br>
                # Rules:<br>
                1. Retain the original text for each step without any modifications, additions, or deletions, only splitting based on logical steps.<br>
                2. If the text does not contain explicit step indications, split the steps according to the logical flow of the content. Each step should have a conclusion progression relative to the previous step and represent a complete intermediate conclusion (e.g., equations, intermediate results, planning thoughts, etc.).<br>
                3. The final output should be a JSON object containing each logical step.<br>
                <br>
                <span style="color: red;">{shot}</span><br>
                <br>
                **Text to process:**<br>
                <br>
                <span style="color: red;">{answer}</span><br>
                <br>
                Please output directly according to the above instructions.
              </div>
            </article>
          </div>
        </div>

        <div id="prompt-dis" class="content-tab is-hidden">
          <div class="content">
            <!-- p0 -->
            <article class="message">
              <div class="message-header">
                <p>LLM discrimination prompt \(p_0\)</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                Your Role and Task:<br>
                <br>
                You are a math teacher, and I need your help in grading exams. I will provide you with the question, standard answer, and student answer. Based on the standard answer, you need to determine the correctness of each step in the student’s answer. The student answer will be given in JSON format, detailing each step of their solution, and you should indicate whether each step is correct or incorrect.<br>
                <br>
                Important Notes:<br>
                1. The student’s answer may have a different approach from the standard answer. If the student’s reasoning is logically sound and their final answer matches the standard answer, then it should be considered correct.<br>
                2. You need to assess each step’s correctness and mark it with 0 for incorrect and 1 for correct. For example, if there are three steps, where the first is correct, and the second and third are incorrect, then at the end of your response, output a list named judge_result like this: judge_result=[1,0,0].The judge_result can only contain 0 and 1.<br>
                3. If a step (step i) is incorrect because of an error in the previous step (step i-1), it should be considered wrong as well, even if the deduction or calculation in step i itself is technically correct.<br>
                4. If a step references an unrelated or inapplicable conclusion (even if the conclusion itself is correct), that step should be considered incorrect.<br>
                5. Critically evaluate all steps and results in your answer, making sure each step has an evaluation conclusion.<br>
                <br>
                The user will provide the question, standard answer, and student answer. Please grade the student answer strictly according to these instructions and include the final judge_result list at the end of your response, like this format: judge_result=[1,0,0]<br>
                <hr>
                <strong>[User]:</strong><br>
                # question:<br>
                <span style="color: red;">{question}</span><br>
                <br>
                # standard answer:<br>
                <span style="color: red;">{ground_truth_solution}</span><br>
                <br>
                # student answer:<br>
                <span style="color: red;">{student_solution}</span><br>
                <br>
                # your output:
              </div>
            </article>

            <!-- p1 -->
            <article class="message">
              <div class="message-header">
                <p>LLM discrimination prompt \(p_1\)</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                # Your Role and Task:<br>
                <br>
                You are a math teacher, and I need your help in grading exams. I will provide you with the question, standard answer, and student answer. Based on the standard answer, you need to determine the correctness of each step in the student’s answer. The student answer will be given in JSON format, detailing each step of their solution, and you should indicate whether each step is correct or incorrect.<br>
                <br>
                # Important Notes:<br>
                1. The student’s answer may have a different approach from the standard answer. If the student’s reasoning is logically sound and their final answer matches the standard answer, then it should be considered correct.<br>
                2. You need to assess each step’s correctness and mark it with 0 for incorrect and 1 for correct. For example, if there are three steps, where the first is correct, and the second and third are incorrect, then at the end of your response, output a list named judge_result like this: judge_result=[1,0,0].<br>
                3. If a step (step i) is incorrect because of an error in the previous step (step i-1), it should be considered wrong as well, even if the deduction or calculation in step i itself is technically correct.<br>
                4. If a step references an unrelated or inapplicable conclusion (even if the conclusion itself is correct), that step should be considered incorrect.<br>
                5. Critically evaluate all steps and results in your answer, making sure each step or results has an evaluation conclusion.<br>
                <br>
                The user will provide questions, model answers, and student answers. Please follow these instructions carefully to grade the student answers. You only need to respond to the final Judge_result list, as example: judge_result=[1,1,1,1].Do not do any additional explanation.<br>
                <br>
                <span style="color: red;">{shot}</span><br>
                <hr>
                <strong>[User]:</strong><br>
                # question:<br>
                <span style="color: red;">{question}</span><br>
                <br>
                # standard answer:<br>
                <span style="color: red;">{ground_truth_solution}</span><br>
                <br>
                # student answer:<br>
                <span style="color: red;">{student_solution}</span><br>
                <br>
                # your output:
              </div>
            </article>

            <!-- p2 -->
            <article class="message">
              <div class="message-header">
                <p>LLM discrimination prompt \(p_2\)</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                Your Role and Task:<br>
                <br>
                You are a math teacher, and I need your help in grading exams. I will provide you with the question, standard answer, and student answer. Based on the standard answer, you need to determine the correctness of each step in the student’s answer. The student answer will be given in JSON format, detailing each step of their solution, and you should indicate whether each step is correct or incorrect.<br>
                <br>
                Important Notes:<br>
                1. The student’s answer may have a different approach from the standard answer. If the student’s reasoning is logically sound and their final answer matches the standard answer, then it should be considered correct.<br>
                2. You need to assess each step’s correctness and mark it with 0 for incorrect and 1 for correct. For example, if there are three steps, where the first is correct, and the second and third are incorrect, then at the end of your response, output a list named judge_result like this: judge_result=[1,0,0].The judge_result can only contain 0 and 1.<br>
                3. If a step (step i) is incorrect because of an error in the previous step (step i-1), it should be considered wrong as well, even if the deduction or calculation in step i itself is technically correct.<br>
                4. If a step references an unrelated or inapplicable conclusion (even if the conclusion itself is correct), that step should be considered incorrect.<br>
                5. Critically evaluate all steps and results in your answer, making sure each step has an evaluation conclusion.<br>
                <br>
                The user will provide the question, standard answer, and student answer. Please grade the student answer strictly according to these instructions and include the final judge_result list at the end of your response, like this format: judge_result=[1,0,0]<br>
                <hr>
                <strong>[User]:</strong><br>
                # question:<br>
                <span style="color: red;">{question}</span><br>
                <br>
                # standard answer:<br>
                <span style="color: red;">{ground_truth_solution}</span><br>
                <br>
                # student answer:<br>
                <span style="color: red;">{student_solution}</span><br>
                <br>
                Your Role and Task:<br>
                <br>
                You are a math teacher, and I need your help in grading exams. I will provide you with the question, standard answer, and student answer. Based on the standard answer, you need to determine the correctness of each step in the student’s answer. The student answer will be given in JSON format, detailing each step of their solution, and you should indicate whether each step is correct or incorrect.<br>
                <br>
                Important Notes:<br>
                1. The student’s answer may have a different approach from the standard answer. If the student’s reasoning is logically sound and their final answer matches the standard answer, then it should be considered correct.<br>
                2. You need to assess each step’s correctness and mark it with 0 for incorrect and 1 for correct. For example, if there are three steps, where the first is correct, and the second and third are incorrect, then at the end of your response, output a list named judge_result like this: judge_result=[1,0,0].The judge_result can only contain 0 and 1.<br>
                3. If a step (step i) is incorrect because of an error in the previous step (step i-1), it should be considered wrong as well, even if the deduction or calculation in step i itself is technically correct.<br>
                4. If a step references an unrelated or inapplicable conclusion (even if the conclusion itself is correct), that step should be considered incorrect.<br>
                5. Critically evaluate all steps and results in your answer, making sure each step has an evaluation conclusion.<br>
                <br>
                The user will provide the question, standard answer, and student answer. Please grade the student answer strictly according to these instructions and include the final judge_result list at the end of your response, like this format: judge_result=[1,0,0]
              </div>
            </article>

            <!-- p3 -->
            <article class="message">
              <div class="message-header">
                <p>LLM discrimination prompt \(p_3\)</p>
              </div>
              <div class="message-body">
                <strong>[System]:</strong><br>
                # Your Role and Task:<br>
                <br>
                You are a math teacher, and I need your help in grading exams. I will provide you with the question, standard answer, and student answer. Based on the standard answer, you need to determine the correctness of each step in the student’s answer. The student answer will be given in JSON format, detailing each step of their solution, and you should indicate whether each step is correct or incorrect.<br>
                <br>
                # Important Notes:<br>
                1. The student’s answer may have a different approach from the standard answer. If the student’s reasoning is logically sound and their final answer matches the standard answer, then it should be considered correct.<br>
                2. You need to assess each step’s correctness and mark it with 0 for incorrect and 1 for correct. For example, if there are three steps, where the first is correct, and the second and third are incorrect, then at the end of your response, output a list named judge_result like this: judge_result=[1,0,0].<br>
                3. If a step (step i) is incorrect because of an error in the previous step (step i-1), it should be considered wrong as well, even if the deduction or calculation in step i itself is technically correct.<br>
                4. If a step references an unrelated or inapplicable conclusion (even if the conclusion itself is correct), that step should be considered incorrect.<br>
                5. Critically evaluate all steps and results in your answer, making sure each step or results has an evaluation conclusion.<br>
                <br>
                <span style="color: red;">{shot}</span><br>
                <br>
                The user will provide questions, model answers, and student answers. Please follow these instructions carefully to grade the student answers. You only need to respond to the final judge_result list, as example: judge_result=[1,1,1,1].Do not do any additional explanation.<br>
                <hr>
                <strong>[User]:</strong><br>
                # question:<br>
                <span style="color: red;">{question}</span><br>
                <br>
                # standard answer:<br>
                <span style="color: red;">{ground_truth_solution}</span><br>
                <br>
                # student answer:<br>
                <span style="color: red;">{student_solution}</span>
              </div>
            </article>

          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<style>
.content-tab {
  animation: fadeIn 0.5s;
}
@keyframes fadeIn {
  from { opacity: 0; }
  to { opacity: 1; }
}
.message-body {
    max-height: 400px;
    overflow-y: auto;
}
/* Fix for active tab text color */
.tabs.is-toggle li.is-active a {
    background-color: #363636;
    border-color: #363636;
    color: #fff !important;
}
.tabs.is-toggle li.is-active a:hover {
    background-color: #363636;
    border-color: #363636;
    color: #fff !important;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const tabs = document.querySelectorAll('.tabs li');
  const tabContents = document.querySelectorAll('.content-tab');

  tabs.forEach(tab => {
    tab.addEventListener('click', () => {
      // Remove active class from all tabs
      tabs.forEach(t => t.classList.remove('is-active'));
      // Add active class to clicked tab
      tab.classList.add('is-active');

      // Hide all tab contents
      tabContents.forEach(content => content.classList.add('is-hidden'));
      // Show target tab content
      const targetId = tab.dataset.tab;
      document.getElementById(targetId).classList.remove('is-hidden');
    });
  });
});
</script>

<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->









<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code class="language-bibtex">
@misc{auroraprm2025,
  author       = {AURORA},
  title        = {AURORA: Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification},
  year         = {2025},
  url          = {https://auroraprm.github.io/}
}
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
